# The continual learning in Simulation-Driven Differentiable Active Learning (SDDAL) framework for efficient data generation

In the initial commit to this Continual-SDDAL repository, an original SDDAL (Simulation-Driven Differentiable Active Learning) framework was pushed into this repository. The following instructures are about how to run different functionalities in the initially committed SDDAL. After the continual researcher is familiar with the initially committed original SDDAL, she will proceed to develop an continual learning scheme inside the SDDAL framework based on the initially pushed SDDAL code base.

# Data and folder preparation

Before running any experiments, the following folder structure must be prepared.

1, Six design folders must be created to contain the intensity and phase in both png images and numpy array files generated by the simulation in SDDAL, the Zernike coefficients used by the simulation to generate the corresponding intensity and phase, the uncertainty maps produced by the Quantile UNet-T model in the SDDAL, and the trained network weights of the Quantile UNet-T model everytime it is retrained on the accumulated training set:
- `Design_rec`
- `Design_ring`
- `Design_chair`
- `Design_gaussian`
- `Design_hat`
- `Design_tear`

Each design folder must share the same internal structure, shown below:

```text
Design_rec/
├── latest_uncertainty/
├── models/
├── training_set/
│   ├── intensity/
│   │   ├── img/
│   │   └── npy/
│   └── phase/
│   │   ├── img/
│   │   └── npy/
│   │── zernikes/
│
└── test_set/
    ├── intensity/
    │   ├── img/
    │   └── npy/
    └── phase/
    │   ├── img/
    │   └── npy/
    │
    │── zernikes/
```

2, For each beam shape, the corresponding test set must be copy-pasted from the original [Processed InShaPe dataset](https://doi.org/10.6084/m9.figshare.30131893.v4) to the test_set subfolder of the corresponding design folder above.

3, To this point, you can already run the SDDAL framework to generate data into the design folder. When you have generated enough data, you will need to officially train an UNet-T model or Quantile UNet-T model on the design-finished training dataset, then test the trained model on the InShaPe test set. To store inference results on test set, another six result folders must be created:
- `rec_result`
- `ring_result`
- `chair_result`
- `gaussian_result`
- `hat_result`
- `tear_result`

Each result folder must share the same internal structure, shown below:

```text
rec_result/
├── Phi_pred/
│   ├── img/
│   └── npy/
├── Phi_gt/
│   ├── img/
│   └── npy/
└── I_gt/
    ├── img/
    └── npy/
```

# Running SDDAL framework for dataset design

The entire pipeline of the simulation-driven differentiable active learning framework is organized in a Shell script (SDDAL.sh), which launches and recycles several implemented python modules periodically including:

- `Initializer.py`: responsible for generating initial training samples from Z<-(-1.5, 1.5) by indicating the number of training samples you desire
- `Trainer.py`: responsible for training the Quantile UNet-T model on the current accumulated training data. After training the Quantile UNet-T will be able to output both predicted phase map and the uncertainty map for it
- `Scanner.py`: responsible for doing batched gradient descent on the PBF-LB/M beam shaping simulation docking its output to the input of the trained weight-frozen Quantile UNet-T with optimization objective as Quantile uncertainty maximum.
- `zernike_statistics.py`: responsible for plotting KDE and histogram curves of the Zernike coefficients found out by SDDAL framework to generate the training dataset.
- `M290_MachineSimu_GPU/M290_full_scene.py`: the differentiable batched Pytorch on-GPU simulation, with Direct Integration as Convolution (DIC) as diffractive propagator, of the PBF-LB/M beam shaping system of the EOS M290 additive manufacturing machine in [EU InShaPe project](https://inshape-horizoneurope.eu/).

- 1, Quick experiment: Execute the following commands in sequence.
  
  Command:
  
  `bash SDDAL.sh rec 0.0002 1000 true 9999 9999 0 9999 9999`
  
   - (1) Create 1000 initial samples by randomly sampling Zernike coefficients from uniform(-1.5, 1.5).

  Command:

  `bash SDDAL.sh rec 9999 9999 false 1 200 0 5 9999 true`
   
   - (2) Train a Quantile UNet model on the 1000 initial samples.

   - (3) SDDAL uses the PBF/LB-M beam shaping simulation and trained Quantile UNet-T for uncertainty sampling to generate 1000 active learned samples.

   - (4) The 1000 initial samples + 1000 active learned samples is the final 2000-sample training set.

  Command:

  `python3 train_unet.py --data Design_rec --epochs 15 --batch_size 2 --gpu 0 --lr 0.0002 --step_size 2 --seed 123 --pth_name rec.pth.tar`

   - (5) Train a randomly initialized UNet-T model on the final 2000-sample training set (GPU data)

  Command:

  `python3 train_unet.py --data Design_rec --batch_size 2 --gpu 1 --seed 123 --pth_name rec.pth.tar --val_vis_path rec_result --eval`

   - (6) Test on the InShaPe test set (CPU data).

- 2, Official dataset design: Execute the following commands in sequence.
 
  Command:
  
  `bash SDDAL.sh rec 0.0002 100 false 1 580 0 5 1 false`
  
   - (1) Create 100 initial samples by randomly sampling Zernike coefficients from uniform(-1.5, 1.5).
 
   - (2) Train a Quantile UNet model on the 100 initial samples.
 
   - (3) SDDAL uses the PBF/LB-M beam shaping simulation and trained Quantile UNet-T for uncertainty sampling with batched-sample generation size 5 to generate 5 active learned samples to be added into the 100 initial samples.
 
   - (4) Retrain the Quantile UNet model on the current_size+5 accumulated samples with the identical randomly initialized network weights to the first time training on 100 initial samples.
 
   - (5) Go back to step (3) and repeat step (3) and (4) until the numer of samples generated satisfies your requirements (in this example, it asks for 3000 training samples in total).

  Command:

  `python3 train_unet.py --data Design_rec --epochs 15 --batch_size 2 --gpu 0 --lr 0.0002 --step_size 2 --seed 123 --pth_name rec.pth.tar`

   - (6) Train a randomly initialized UNet-T model on the final 2000-sample training set (GPU data)

  Command:

  `python3 train_unet.py --data Design_rec --batch_size 2 --gpu 1 --seed 123 --pth_name rec.pth.tar --val_vis_path rec_result --eval`

   - (7) Test on the InShaPe test set (CPU data).

# Evaluate the performance of the generated training dataset

- 1, Train UNet-T model from scratch on the generated training set. As already indicated in the previous section, for any generated dataset residing in any design folder, you should run the following command to train UNet-T on it:
```text
python3 train_unet.py --data Design_rec --epochs 15 --batch_size 2 --gpu 0 --lr 0.0002 --step_size 2 --seed 123 --pth_name rec.pth.tar
```
- 2, Test trained UNet-T model on InShaPe (CPU) test set. As already indicated in the previous section, you should run the following command for inference on test set:
```text
python3 train_unet.py --data Design_rec --batch_size 2 --gpu 1 --seed 123 --pth_name rec.pth.tar --val_vis_path rec_result --eval
```
- 3, The predicted phase map, the ground-truth phase map, and the ground-truth intensity are stored in ./rec_result for RecTophat shape for example. Now switch into the result folder:
```text
cd ./rec_result/
```
- 4, Inside the result folder you will see a Python script named "FRCM.py", please create a new folder named "diff" in the current directory, and then run the following command:
